import asyncio
try:
    asyncio.get_event_loop() #Check if event loop already exists
except RuntimeError: #phÃ²ng trÆ°á»ng há»£p event táº¡o á»Ÿ 1 main thread khÃ¡c
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
import streamlit as st
from src.qa_chain import get_chain, debug_memory
from src.utils import get_embedding_model

embedding_model = get_embedding_model()

#sidebar Ä‘iá»u chá»‰nh kawrgs, temp
with st.sidebar:
    st.header("âš™ï¸ TÃ¹y chá»‰nh tham sá»‘")

    k_slider = st.slider(
        "Sá»‘ lÆ°á»£ng chunk tÃ¬m kiáº¿m: (k):",
        min_value = 1,
        max_value = 10,
        value = 3,
        step = 1,
    )
    temperature_slider = st.slider(
        "Temperature:",
        min_value = 0.0,
        max_value = 1.0,
        value = 0.1,
        step = 0.1,
    )

    st.info("ÄÃ¢y lÃ  nhá»¯ng gÃ¬ bot Ä‘ang nhá»› hiá»‡n táº¡i")

    current_session_id = "user_vjp_pro_1"

    memory_content = debug_memory(current_session_id)
    st.json(memory_content)

    if st.button("ğŸ—‘ï¸ XÃ³a TrÃ­ Nhá»› (Clear RAM)"):
        from src.qa_chain import store
        if current_session_id in store:
            del store[current_session_id]
            st.rerun()
    

k_value = k_slider
temperature_value = temperature_slider

def stream_handler(chain, question, session_id):

    #input(å…¥åŠ›) pháº£i lÃ  Dict (è¾æ›¸å‹), gá»“m answer, context, nÃªn ta pháº£i tÃ¡ch answer cho Streamlit hiá»ƒn thá»‹
    stream = chain.stream(
        {"question": question},
        config={"configurable": {"session_id": session_id}}
    )

    #Biáº¿nï¼ˆå¤‰æ•°) dÃ¹ng Ä‘á»ƒ lÆ°u láº¡i nguá»“n (å‚ç…§å…ƒã®ä¿å­˜) (do nguá»“n thÆ°á»ng tráº£ vá» 1 cá»¥c, khÃ´ng stream tá»«ng chá»¯) 
    full_context = None

    for chunk in stream:
        if "context" in chunk:
            full_context = chunk["context"]

        if "answer" in chunk:
            yield chunk["answer"] #stream tá»«ng chá»¯ cho streamlit

    st.session_state.last_context = full_context
    


def load_chain(k,temperature):
    return get_chain(k = k, temperature = temperature, embedding_model = embedding_model)

rag_chain = load_chain(k = k_value, temperature = temperature_value)

#Tach rieng 2 initialization: 

#2 - Day moi la list chua context cua cau hoi cuoi cung (LLM memory)
if "last_context" not in st.session_state:
    st.session_state.last_context = []

def handle_query(question):
    with st.chat_message("user"):
        st.markdown(question)

    st.session_state.messages.append({"role": "user", "content": question})

    with st.chat_message("ai"):

        session_id = "user_vjp_pro_1"

        #dÃ¹ng st.write_stream Ä‘á»ƒ nháº­n generator 'yield' á»Ÿ trÃªn
        full_response = st.write_stream(stream_handler(rag_chain, question, session_id))

        sources = st.session_state.get("last_context", [])
        if sources:
            st.divider() #Ke 1 duong phan cach
            st.subheader("ğŸ“š Nguá»“n tÃ i liá»‡u tham kháº£o")
            for i, doc in enumerate(sources):
                source_name = doc.metadata.get("title", f"Nguá»“n tÃ i liá»‡u #{i+1}")

                with st.expander(f"ğŸ“– [{i+1}] {source_name}"):
                    #highlight important keyword
                    st.markdown(f"**Ná»™i dung**")
                    st.info(doc.page_content)

    #luu cau tra loi cua AI vao history de hien thi
    st.session_state.messages.append({
        "role": "ai", 
        "content": full_response,
        "sources": sources #avoid losing sources when reload
    })
#Init state
if "messages" not in st.session_state:
    st.session_state.messages = []

#CHI ve 1 an duy nhat - tranh loi double display
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

        if "sources" in msg and msg["sources"]:
            st.divider() #Ke 1 duong phan cach
            st.subheader("ğŸ“š Nguá»“n tÃ i liá»‡u tham kháº£o")
            for i, doc in enumerate(msg["sources"]):
                source_name = doc.metadata.get("title", f"Nguá»“n tÃ i liá»‡u #{i+1}")

                with st.expander(f"ğŸ“– [{i+1}] {source_name}"):
                    #highlight important keyword
                    st.markdown(f"**Ná»™i dung**")
                    st.info(doc.page_content)


#render UI
if not st.session_state.messages:
    #HERO SECTION: display when no messages are found

    st.markdown("<br><br>", unsafe_allow_html=True)

    st.markdown("""
    <div style = "text-align: center;">
        <h1>ğŸ¤– HUST Regulations Bot </h1>
        <p> Trá»£ lÃ½ AI há»— trá»£ tra cá»©u Quy cháº¿ Ä‘Ã o táº¡o ÄHBK HÃ  Ná»™i. </p>
        <p style= "color: grey; font-sizze: 0.9em;"> ğŸ‘‹ ChÃ o má»«ng báº¡n Ä‘áº¿n vá»›i trá»£ lÃ½ AI! Dá»¯ liá»‡u dá»±a trÃªn vÄƒn báº£n há»£p nháº¥t 2025
        , náº¿u báº¡n cÃ³ báº¥t kÃ¬ cÃ¢u há»i nÃ o vá» quy cháº¿, hoáº·c Ä‘Æ¡n giáº£n lÃ  muá»‘n nÃ³i chuyá»‡n vui váº», trÃ² chuyá»‡n,
        mÃ¬nh sáº½ sáºµn sÃ ng há»— trá»£!</p>
    </div>
    """, unsafe_allow_html=True)

    st.markdown("<br>", unsafe_allow_html=True)

    #Tao 2 cot cho nut goi y
    col1, col2 = st.columns(2)


    suggestions = [
        "CÃ¡ch tÃ­nh Ä‘iá»ƒm há»c pháº§n",
        "Äiá»u kiá»‡n nháº­n há»c bá»•ng KKHT",
        "Quy Ä‘á»‹nh vá» nghá»‰ há»c táº¡m thá»i",
        "Há»c pháº§n song hÃ nh lÃ  gÃ¬"
    ]

    def set_prompt(text):
        st.session_state.prompt_trigger = text

    with col1:
        if st.button(suggestions[0], use_container_width=True):
            handle_query(suggestions[0])
            st.rerun()
        
        if st.button(suggestions[2], use_container_width=True):
            handle_query(suggestions[2])
            st.rerun()

    with col2:
        if st.button(suggestions[1], use_container_width=True):
            handle_query(suggestions[1])
            st.rerun()
        
        if st.button(suggestions[3], use_container_width=True):
            handle_query(suggestions[3])
            st.rerun()

#input handling

#First - kiem tra trigger tu button (uu tien 1)
if "prompt_trigger" in st.session_state:
    prompt = st.session_state.prompt_trigger
    del st.session_state.prompt_trigger
    handle_query(prompt)
    st.rerun()


#Roi moi kiem tra chat input UI (uu tien 2)
#placeholder bang tieng viet
elif prompt := st.chat_input("Nháº­p cÃ¢u há»i vá» quy cháº¿, hoáº·c chat chit..."):
    handle_query(prompt)
    st.rerun()